# VQA-Detecting-Ambiguity-and-Generating-Multiple-Answer-Candidates-for-Clarifying-Visual-Questions
# Introduction
This project explores ambiguity detection in Visual Question Answering (VQA) tasks. We propose a rule-enhanced binary classification approach to identify ambiguous questions based on entropy, visual similarity. Our method is tested on both the SIMMC 2.1 dataset.

This repository includes:
Rule-based ambiguity scoring method
Ambiguity classifier trained on interpretable features
Evaluation code (precision, recall, F1, accuracy)

# References
Please cite or acknowledge the following works if you use this code:

SIMMC 2.1 Dataset:
Moon et al., "Multimodal Simultaneous Co-reference Resolution and Response Generation in Task-oriented Dialog," EMNLP 2020
https://github.com/facebookresearch/simmc2.0

ViLBERT (used in original feature extraction):
Lu et al., "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks," NeurIPS 2019
https://github.com/facebookresearch/vilbert-multi-task

If you use this repository in your research or paper, please cite it accordingly.

# Author
This project is part of my research on multimodal ambiguity detection. For feedback or collaboration, feel free to open an issue or contact me.
